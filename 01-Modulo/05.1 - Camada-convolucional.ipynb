{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01.1 - Camada-convolucional.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-nynttjfWsXN","colab_type":"text"},"source":["# Camada Convolucional\n","\n","Como vimos, as camadas densas tem severos problemas, principalmente para trabalhar com imagens, pois a dimensionalidade da entrada é muito grande o que implica num alto número de parâmetros para se otimizar.\n","\n","Contornando essa situação, temos as camadas convolucionais.\n","Ao longo do tempo, várias tipos de camadas convolucionais foram criadas.\n","Nesta aula veremos algumas dessas camadas.\n"]},{"cell_type":"markdown","metadata":{"id":"qcZHKsUmWvxN","colab_type":"text"},"source":["Antes de começar, vamos instalar o Pytorch. Esse pequeno bloco de código abaixo é usado somente para instalar o Pytorch para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"code","metadata":{"id":"kjO1spZMUot_","colab_type":"code","outputId":"fb2abb32-bf5c-4a5b-e4cf-f056bde19c84","executionInfo":{"status":"ok","timestamp":1570923850725,"user_tz":180,"elapsed":4413,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!pip3 install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nAryl7i7U1lp","colab_type":"code","colab":{}},"source":["import time, os, sys, numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch import optim\n","\n","import time, os, sys, numpy as np\n","\n","# Test if GPU is avaliable, if not, use cpu instead\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","n = torch.cuda.device_count()\n","devices_ids= list(range(n))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HARXNqVHVaT4","colab_type":"code","colab":{}},"source":["def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n","    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [torchvision.transforms.Resize(resize)]\n","    transformer += [torchvision.transforms.ToTensor()]\n","    transformer = torchvision.transforms.Compose(transformer)\n","\n","    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True,download=True,transform=transformer)\n","    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False,download=True,transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","\n","\n","    train_iter = torch.utils.data.DataLoader(mnist_train,\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = torch.utils.data.DataLoader(mnist_test,\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","\n","# funções básicas\n","def _get_batch(batch):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.type() != features.type():\n","        labels = labels.type(features.type())\n","    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n","            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","\n","    acc_sum, n, l = torch.Tensor([0]), 0, 0\n","    \n","    with torch.no_grad():\n","      for X, y in data_iter:\n","          #y = y.astype('float32')\n","          X, y = X.to(device), y.to(device)\n","          y_hat = net(X)\n","          l += loss(y_hat, y).sum()\n","          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","          n += y.size()[0]\n","\n","    return acc_sum.item() / n, l.item() / len(data_iter)\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n","                   num_epochs):\n","    print('training on', device)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","            trainer.zero_grad()\n","            l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step()\n","            train_l_sum += l.item()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","            n += y.size()[0]\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKPEdbQFrDA","colab_type":"text"},"source":["## Camada Convolucional\n","\n","A [camada convolucional](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv2D.html) é considerado o principal módulo das rede convolucionais (*ConvNet*) pois é responsável por fazer a maior parte do trabalho, ou seja, aprendar filtros que extraem características.\n","\n","Tecnicamente, uma camada convolucional é composta por um conjunto de filtros (ou *kernels*) aprendíveis (que, na verdade, representam os parâmetros dessa camada).\n","Geralmente, cada um desses filtros é relativamente pequeno (em termos de largura e altura), mas se estende por toda a profundidade dos dados de entrada.\n","Por exemplo, cada filtro em uma primeira camada de uma *ConvNet* geralmente tem tamanho $3 \\times h\\times w$, onde $h$ e $w$ são altura e largura, respectivamente, e o $3$ representa a profundidade dos filtros que, neste caso, são ligados aos canais de cores da entrada (RGB).\n","Num primeiro momento, trabalharemos somente com filtros bidimensionais pois deixaremos de lado a dimensão relacionada aos canais.\n","Durante o *forward*, convolucionamos cada filtro sobre a entrada calculando o produto entre esses valores e gerando, como saída, um mapa de ativação bidimensional que fornece as respostas desse filtro em todas as possíveis posições.\n","Em outras palavras, cada filtro pode ser visto como um neurônio que irá combinar os canais de entradas gerando uma saída.\n","Intuitivamente, a rede aprenderá filtros que são ativados quando encontram algum tipo de características visual interessante, como uma borda na primeira camada, ou eventualmente padrões mais complexos nas camadas mais finais da rede.\n","Cada camada convolucional é composta por conjuntos inteiro de filtros onde, cada um deles, produzirá um mapa de ativação bidimensional separado.\n","Esses mapas de ativação são empilhados (na profundidade) produzindo a saída final (comumente chamada de *feature maps*).\n","\n","Formalmente, o procedimento de convolução 2-D, comumente empregado nesse tipo de camada, recebe uma entrada de duas dimensões $x$ e um vetor de peso 2-D $K$ (nesse caso, com tamanho $n\\times n$) e os processa da seguinte forma:\n","\n","$$ Y[k, l] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} X(k + i - 1, l + j -1) K(i,j) $$\n",", onde $Y$ é a saída ou *feature map*.\n","\n","Abaixo temos um exemplo de convolução.\n","Neste caso, a entrada é uma matriz bidimensional com uma altura de 3 e largura de 3 ($3 \\times 3$ e o filtro tem dimensões $2\\times 2$.\n","Este exemplo destaca (em azul) um passo do processo de convolução.\n","Entretanto, como dito anteriormente, a janela (filtro) de convolução percorre toda a entrada ao longo da altura e largura, sempre multiplicando e somando os elementos da entrada pelos valores do filtro.\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1JSTFGFpfjdMVvkaHUUwzGlgle6UVhFjo\">\n","</p>\n","\n","O exemplo acima temos somente **um** canal de entrada e de saída. Porém, como funciona a convolução com múltiplos canais de entrada e saída?\n","\n","### Múltiplos canais de entrada e saída\n","\n","\n","No exemplo anterior, tanto os filtros quanto as saídas podem ser vistas como matrizes bidimensionais.\n","Entretanto, quando adicionamos mais canais, entradas, filtros, e saídas passam a ser representadas como matrizes tridimensionais.\n","Por exemplo, cada imagem de entrada RGB tem a forma $3 \\times h \\times w$, onde $h$ e $w$ são a altura e largua respectivamente..\n","A dimensão com um tamanho de 3 é referenciada como a dimensão do canal.\n","\n","#### Múltiplos Canais de Entrada\n","\n","Quando os dados de entrada contêm múltiplos canais, precisamos de filtros de convolução com o mesmo número de canais que os dados de entrada, para que ele possamos processar uma convolução.\n","Assumindo que o número de canais para os dados de entrada é $c_i$, o número de canais dos filtro de convolução também precisa ser $c_i$.\n","Quando $c_i = 1$, o filtro de convolução é uma uma matriz bidimensional com dimensões $k_h \\times k_w$.\n","\n","No entanto, quando $ c_i> 1$, precisamos de um filtro que contenha uma matriz $k_h \\times k_w$ **para cada canal de entrada**.\n","Concatenanado todos esses $c_i$ arrays juntos geramos um filtro de convolução com dimensões $c_i \\times k_h \\times k_w$.\n","Como a entrada e o filtro da convolução tem cada um $c_i$ canais, podemos criar uma correspondência entre canais para executar a operação de convolução.\n","Em outras palavras, podemos fazer a convolução da matriz bidimensional da entrada com o *kernel* bidimensional para cada canal, somando os resultados ao longo dos canais $c_i$, produzindo uma só matriz bidimensional de saída.\n","\n","Na figura abaixo, demonstramos um exemplo de convolução com dois canais de entrada.\n","As partes sombreadas representam o primeiro elemento de saída, bem como os elementos de entrada e de matriz do kernel usados em sua computação: $(1 \\times1 + 2 \\times 2 + 4 \\times 3 + 5 \\times 4) + (0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3) = 56$.\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1OC-o75LRvxi8KllotBoR8cS38NS4EOTm\">\n","</p>\n","\n","### Múltiplos canais de saída\n","\n","Independentemente do número de canais de entrada, até agora nós sempre acabamos com um canal de saída.\n","No entanto, aumentar o número de canais (neurônios) em cada camada implica em aumentar a poder de representação daquela camada.\n","Nas arquiteturas de redes neurais mais populares, na verdade aumentamos a dimensão do canal à medida que avançamos na rede neural, geralmente diminuindo a resolução. \n","\n","Denote por $c_i$ e $c_o$ o número de canais de entrada e saída, respectivamente, e deixe que $k_h$ e $k_w$ sejam a altura e a largura do filtro convolucional.\n","Para obter uma saída com múltiplos canais, podemos criar uma matriz de kernel de forma $c_i \\times k_h \\times k_w $ para cada canal de saída $c_o$ (que também pode ser visto como neurônio).\n","Concatenamos na dimensão do canal de saída, para que o filtro de convolução tenha resolução final de $c_o \\times c_i \\times k_h \\times k_w$.\n","Nas operações de convolução, o resultado em cada canal de saída é calculado a partir do filtro de convolução correspondente a esse canal de saída e recebe a entrada de todos os canais na matriz de entrada.\n","\n","Para ter uma visão mais ampla da convolução em múltiplos canais de entrada e saída, acesse esse [site](http://cs231n.github.io/convolutional-networks/) e procurem pela gif relacionada ao tema."]},{"cell_type":"markdown","metadata":{"id":"GfNSNOsYF25-","colab_type":"text"},"source":["### Pytorch e o caso de estudo LeNet-5\n","\n","Frameworks modernos implementam camadas convolucionais de forma fácil e intuitiva.\n","No Pytorch, a [camada de convolução](https://pytorch.org/docs/stable/nn.html#conv2d) tem alguns dos parâmetros que vimos anteriormente na sua declaração.\n","\n","Vamos implementar uma rede baseada na [LeNet-5](https://ieeexplore.ieee.org/document/726791) e entender cada parâmetro.\n","A rede tem essa arquitetura (ignorem, nesse primeiro momento, os *subsamplings*):\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://miro.medium.com/max/2625/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n","</p>\n","\n","As camadas com a letra C, são convoluções. Camadas que começam com a letra S, são *subsampling* e devem ser ignoradas neste primeiro momento.\n","Já camada com ínicio F, são *fully-connected*.\n","Abaixo, uma tabela que compila toda a configuração da rede.\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://engmrk.com/wp-content/uploads/2018/09/LeNEt_Summary_Table.jpg\">\n","</p>\n","\n","Abaixo, recriamos a rede no Pytorch sem as camadas de *subsampling*.\n"]},{"cell_type":"code","metadata":{"id":"nHvdPTSYj7pk","colab_type":"code","colab":{}},"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.01)\n","    if classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.01)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMisije7F0Jx","colab_type":"code","outputId":"071fff0b-3aca-4622-a5c8-56a78b1881a2","executionInfo":{"status":"ok","timestamp":1570924144613,"user_tz":180,"elapsed":298262,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n","# tamanho do batch, e lambda do weight decay\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.1, 128, 0.000001\n","\n","# rede baseada na LeNet-5 \n","net = nn.Sequential(\n","nn.Conv2d(in_channels=1,out_channels=6, kernel_size=5),     # entrada: 1 canal e saida: 6 canais\n","        nn.Tanh(),  \n","        nn.Conv2d(in_channels=6,out_channels= 16, kernel_size=5),    # entrada: 6 canais e saida: 16 canais\n","        nn.Tanh(),  \n","        nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5),   # entrada: 16 canais e saida: 120 canais\n","        nn.Tanh(),  \n","        nn.Flatten(),  # lineariza formando um vetor        # entrada: 120 canais e saida: linear\n","        nn.Linear(48000, 84),\n","        nn.Tanh(),  \n","        nn.Linear(84, 10)\n","     ) \n","\n","# Não é necessário inicializar \n","# net.apply(weights_init)\n","\n","# Sending model to device\n","net.to(device)\n","\n","# função de custo (ou loss)\n","loss = nn.CrossEntropyLoss()\n","\n","# carregamento do dado: mnist\n","train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=32)\n","\n","# trainer do pytorch\n","trainer = optim.SGD(net.parameters(), lr=0.01, weight_decay=wd_lambda)\n","\n","# treinamento e validação via Pytorch\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:02, 13155736.85it/s]                             \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 95265.10it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["4423680it [00:01, 3921171.63it/s]                             \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 31594.12it/s]            "],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/datasets/fashion-mnist/FashionMNIST/raw\n","Processing...\n","Done!\n","training on cuda\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 1, train loss 0.9058, train acc 0.701, test loss 0.6417, test acc 0.762, time 14.5 sec\n","epoch 2, train loss 0.5700, train acc 0.790, test loss 0.5495, test acc 0.797, time 14.0 sec\n","epoch 3, train loss 0.5106, train acc 0.815, test loss 0.5151, test acc 0.811, time 14.2 sec\n","epoch 4, train loss 0.4749, train acc 0.830, test loss 0.4895, test acc 0.826, time 14.1 sec\n","epoch 5, train loss 0.4485, train acc 0.840, test loss 0.4661, test acc 0.829, time 14.3 sec\n","epoch 6, train loss 0.4255, train acc 0.849, test loss 0.4454, test acc 0.840, time 14.0 sec\n","epoch 7, train loss 0.4080, train acc 0.856, test loss 0.4375, test acc 0.842, time 14.0 sec\n","epoch 8, train loss 0.3916, train acc 0.861, test loss 0.4344, test acc 0.843, time 14.0 sec\n","epoch 9, train loss 0.3782, train acc 0.866, test loss 0.4039, test acc 0.853, time 14.0 sec\n","epoch 10, train loss 0.3651, train acc 0.870, test loss 0.4086, test acc 0.850, time 14.0 sec\n","epoch 11, train loss 0.3549, train acc 0.874, test loss 0.3890, test acc 0.860, time 14.1 sec\n","epoch 12, train loss 0.3448, train acc 0.878, test loss 0.3761, test acc 0.866, time 14.2 sec\n","epoch 13, train loss 0.3360, train acc 0.881, test loss 0.3701, test acc 0.870, time 14.3 sec\n","epoch 14, train loss 0.3272, train acc 0.883, test loss 0.3612, test acc 0.873, time 14.2 sec\n","epoch 15, train loss 0.3198, train acc 0.886, test loss 0.3602, test acc 0.870, time 14.2 sec\n","epoch 16, train loss 0.3126, train acc 0.889, test loss 0.3514, test acc 0.871, time 14.0 sec\n","epoch 17, train loss 0.3040, train acc 0.892, test loss 0.3603, test acc 0.867, time 14.2 sec\n","epoch 18, train loss 0.2987, train acc 0.894, test loss 0.3451, test acc 0.875, time 14.1 sec\n","epoch 19, train loss 0.2921, train acc 0.896, test loss 0.3429, test acc 0.872, time 14.1 sec\n","epoch 20, train loss 0.2855, train acc 0.898, test loss 0.3363, test acc 0.878, time 14.1 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZXXLcXlDlr-U","colab_type":"text"},"source":["### Hiper-parâmetros: *Padding* e *Stride*\n","\n","No exemplo anterior, a entrada tinha dimensões $6\\times8$ e o filtro de convolução $1\\times2$.\n","O processamento da convolução produziu, então, uma saída com uma resolução $6\\times7$. \n","Esse diferença da resolução é motivada pelo próprio processamento da convolução e da sua forma de lidar com as extremidades da imagem.\n","Em geral, assumindo que a entrada tem tamanho $n_h\\times n_w$ e o filtro de convolução tem dimensões $k_h \\times k_w$, então o tamanho da saída pode ser calculado da seguinte forma:\n","\n","$$ (n_h-k_h + 1) \\times (n_w-k_w + 1)$$\n","\n","Neste caso, as dimensões da saída é determinada pelos tamanhos da entrada e do filtro de convolução.\n","\n","Em alguns casos, podemos incorporar técnicas comum de processamento de imagem (como *Padding* e *Stride*) que afetam diretamente o tamanho da saída:\n","\n","* Em geral, como filtros geralmente têm dimensões maiores que 1, após muitas convoluções sucessivas, a saída termina ficando muito menor do que a entrada.\n","Por exemplo, imagine que uma imagem de entrada com $240\\times 240$ pixels seja processada por 10 camadas de convoluções $5\\times 5$.\n","Neste caso, a imagem inicial será reduzida para uma saída de $200\\times 200$ pixels, ou seja,  30% da imagem original é eliminanda e, com ela, informações interessante próxima das extremidades da imagem de entrada. *Padding* lida com esse problema. \n","* Em alguns casos, como quando a entrada tem uma resolução muito grande, queremos reduzir drasticamente a resolução da imagem durante o seu processamento. *Strides* podem ajudar nesses casos.\n","\n","#### *Padding*\n","\n","Como descrito acima, um problema complicado ao se trabalhar com camadas convolucionais é a perda de pixels (e, consequentemente, informação) na extremidade da imagem.\n","Como normalmente usamos filtros pequenos, a perda num geral é pequena.\n","Entretanto, ela se torna maior à medida que aplicamos várias camadas convolucionais sucessivas.\n","Uma solução direta para esse problema é adicionar pixels extras ao redor da imagem de entrada, de forma a aumentar o tamanho efetivo da imagem.\n","Esse processo é conhecido como *padding*.\n","Normalmente, definimos os valores desses pixels extras como 0 (*zero-padding*).\n","Abaixo, temos um exemplo visual de uma entrada $3\\times 3$ com *padding* de tamanho 1 em todos os lados sendo processada por um *kernel* $2\\times 2$.\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1Y66FJerJtlyQZGNgf_JzG5LLPsYrmREH\">\n","</p>\n","\n","Para ficar ainda mais claro o funcionamento do *padding*, implementamos esse processo usando o trecho de código abaixo.\n","Neste caso, a entrada, de tamanho $3\\times 3$, tem sua resolução aumentada  $5\\times 5$ usando *padding*.\n","Dessa forma, a saída correspondente é também aumentada para $4\\times 4$."]},{"cell_type":"code","metadata":{"id":"BsIVTT0_gSSK","colab_type":"code","outputId":"78738bdb-b11f-4594-827a-2767df301dee","executionInfo":{"status":"ok","timestamp":1570924144620,"user_tz":180,"elapsed":298252,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["n = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([0.1]))\n","X = n.sample((1,1,3,3))\n","# adicionando o padding\n","X_pad = F.pad(X, mode='constant', pad=(0,0,1,1,1,1,0,0,0,0), value=0)  # Faz 0 pad da última dimension, 1 pad pra cada lada da penúltimo dimension\n","                                                                       # 1 pad pra cada lado da anti-penúltima dimension e 0 para outras\n","\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=2)\n","\n","print('-----Entrada original-----')\n","print(torch.squeeze(X))\n","print('\\n-----Entrada Padding-----')\n","print(torch.squeeze(X_pad))\n","\n","print('\\n-----Saida com entrada original-----')\n","print(conv2d(X.view(1,1,3,3)))\n","print('\\n-----Saida com entrada padding-----')\n","print(conv2d(X_pad.view(1,1,5,5)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["-----Entrada original-----\n","tensor([[ 0.0130,  0.0105,  0.1495],\n","        [-0.0895,  0.1255,  0.0794],\n","        [ 0.0396, -0.1081, -0.1174]])\n","\n","-----Entrada Padding-----\n","tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0130,  0.0105,  0.1495,  0.0000],\n","        [ 0.0000, -0.0895,  0.1255,  0.0794,  0.0000],\n","        [ 0.0000,  0.0396, -0.1081, -0.1174,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n","\n","-----Saida com entrada original-----\n","tensor([[[[0.4509, 0.3543],\n","          [0.4444, 0.5421]]]], grad_fn=<MkldnnConvolutionBackward>)\n","\n","-----Saida com entrada padding-----\n","tensor([[[[0.4801, 0.4782, 0.4215, 0.4522],\n","          [0.5161, 0.4509, 0.3543, 0.4947],\n","          [0.5125, 0.4444, 0.5421, 0.5259],\n","          [0.4663, 0.5449, 0.5228, 0.4643]]]],\n","       grad_fn=<MkldnnConvolutionBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8cZ5WFOGwl9k","colab_type":"text"},"source":["Em geral, se adicionarmos um total de $p_h$ linhas de *padding* (aproximadamente metade na parte superior e metade na parte inferior) e um total de $p_w$ colunas de *padding* (aproximadamente metade à esquerda e metade à direita da entrada), as dimensões da saída serão calculadas da seguinte forma:\n","\n","$$ (n_h-k_h + p_h + 1) \\times (n_w-k_w + p_w + 1) $$\n","\n","Isso significa que a altura e a largura da saída aumentarão em $p_h$ e $p_w$, respectivamente.\n","\n","Em muitos casos, definiremos $p_h = k_h-1$ e $p_w = k_w-1$ para termos a entrada e saída com as mesmas dimensões.\n","Isso facilitará o cálculo da dimensão da saída de cada camada ao construir a rede.\n","Assumindo que $k_h$ é ímpar, podemos preencher $p_h/2$ linhas nos dois lados da altura.\n","Se $k_h$ for par, uma possibilidade é preencher $\\lceil p_h/2 \\rceil $ linhas na parte superior da entrada e $\\lfloor p_h/2 \\rfloor$ linhas na parte inferior.\n","A largura é tratada da mesma maneira.\n","\n","Redes neurais convolucionais comumente usam filtros convolucionais com valores ímpares de altura e largura, como 1, 3, 5 ou 7.\n","Escolher tamanhos ímpares de *kernel* tem o benefício de preservar a dimensionalidade espacial em relação ao *padding*, ou seja, o mesmo número de linhas e colunas serão adicionadas em todos os lados da entrada.\n","\n","No exemplo a seguir, criamos uma camada convolucional com filtro de altura e largura iguais à 3 e aplicamos *padding* de 1 pixel em todos os lados do dado de entrada.\n","Logo, dada uma entrada com resolução $8\\times 8$, temos que a altura e a largura da saída também serão 8."]},{"cell_type":"code","metadata":{"id":"GTBwwQXRnCvj","colab_type":"code","outputId":"01893e09-ddf5-46dd-a6db-8fdf759ae0aa","executionInfo":{"status":"ok","timestamp":1570924144622,"user_tz":180,"elapsed":298241,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["n = torch.distributions.Uniform(torch.tensor([0.0]),torch.tensor([1.0]))\n","X = n.sample((8,8))\n","\n","# Por conveniência, definimos uma função para calcular a camada convolucional.\n","# Esta função inicializa os pesos da camada convolucional e executa\n","# modificacoes correspondentes de dimensionalidade na entrada e saída\n","def comp_conv2d(conv2d, X):\n","    # (1,1) indica o tamanho do batch e a quantidade de canais\n","    X = X.reshape((1, 1) + X.shape)\n","    Y = conv2d(X.view(1,1,8,8))\n","    # exclui as duas primeiras dimensoes que nao nos interessam\n","    return Y.reshape(Y.shape[2:])\n","\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=3, padding=0)\n","print(comp_conv2d(conv2d, X).shape)\n","  \n","# Note que aqui 1 linha ou coluna é coloca em ambos os lados,\n","# então um total de 2 linhas ou colunas são adicionadas\n","conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n","print(comp_conv2d(conv2d, X).shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["torch.Size([6, 6])\n","torch.Size([8, 8])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aT59grJ-yunx","colab_type":"text"},"source":["Quando a altura e a largura do kernel de convolução são diferentes, podemos fazer com que a saída e a entrada tenham a mesma altura e largura definindo diferentes valores de *padding*  para altura e largura."]},{"cell_type":"code","metadata":{"id":"ddK1fA8Vw---","colab_type":"code","outputId":"84afda63-d81d-4b04-f9b6-a2be656a7cb4","executionInfo":{"status":"ok","timestamp":1570924144624,"user_tz":180,"elapsed":298228,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Aqui, usamos um kernel de convolução com uma altura de 5 e uma largura de 3\n","# O *padding* na dimensão da altura eh 2 enquanto que,\n","# na dimensao da largura, eh 1\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n","comp_conv2d(conv2d, X).shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 8])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"6R4-Bsq0zJo7","colab_type":"text"},"source":["#### *Stride*\n","\n","Ao calcular convolução, começamos com a janela (filtro) no canto superior esquerdo da matriz de entrada, e, em seguida, o deslizamos por todos os locais, para baixo e para a direita.\n","Anteriormente, sempre realizamos um passo de um pixel por vez.\n","No entanto, às vezes, seja por eficiência computacional ou porque queremos reduzir o tamanho da entrada, podemos mover a janela mais de um pixel de cada vez,  pulando e igorando muitos pixels de uma vez.\n","\n","Referimo-nos ao número de linhas e colunas percorridas por um passo como *stride*.\n","Até agora, usamos *strides* de 1, tanto para altura quanto largura.\n","Às vezes, podemos querer usar um valor maior de *stride*.\n","Logicamente, que o valor do *stride* impacta diretamente na saída.\n","Em geral, quando o *stride* para a altura é $s_h$ e o *stride* para a largura é $s_w$, o tamanho da saída será calculada da seguinte forma:\n","\n","$$\\lfloor (n_h-k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor(n_w-k_w + p_w + s_w) / s_w \\rfloor$$\n","\n","Se definirmos $p_h = k_h-1$ e $p_w = k_w-1$, então o cálculo da saída será simplificado para $\\lfloor (n_h + s_h-1) /s_h \\rfloor \\times \\lfloor(n_w + s_w-1) /s_w \\rfloor$.\n","\n","A figura abaixo mostra uma operação de convolução com *stride* de 3 verticalmente e 2 horizontalmente.\n","Podemos ver que quando o segundo elemento da primeira coluna é gerado, a janela de convolução desliza três linhas.\n","A janela de convolução desliza duas colunas para a direita quando o segundo elemento da primeira linha é gerado..\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1roGo9atDF5l2ocf9as0L_FDeMDxnAyuL\">\n","</p>\n","\n","Abaixo, um exemplo comparando o tamanho da saída quando usamos *stride* (1, 1), (2, 2), e (3, 4) para altura e largura, respectivamente.\n","Notem que, com a mesma entrada X de tamanho $8\\times8$ a saída fica bem diferente."]},{"cell_type":"code","metadata":{"id":"PeA23S38zHt-","colab_type":"code","outputId":"2207ad89-daee-4ce7-b03c-12758b39cfc7","executionInfo":{"status":"ok","timestamp":1570924144625,"user_tz":180,"elapsed":298218,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["n = torch.distributions.Uniform(torch.tensor([0.0]),torch.tensor([1.0]))\n","X = n.sample((8,8))\n","\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=3, padding=1, stride=1)\n","print('-----Stride 1, Padding 1-----')\n","print(comp_conv2d(conv2d, X).shape)\n","\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=3, padding=1, stride=2)\n","print('\\n-----Stride 2, Padding 1-----')\n","print(comp_conv2d(conv2d, X).shape)\n","\n","conv2d = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n","print('\\n-----Stride (3,4), Padding (0,1)-----')\n","print(comp_conv2d(conv2d, X).shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["-----Stride 1, Padding 1-----\n","torch.Size([8, 8])\n","\n","-----Stride 2, Padding 1-----\n","torch.Size([4, 4])\n","\n","-----Stride (3,4), Padding (0,1)-----\n","torch.Size([2, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zO-fSEYPztrv","colab_type":"text"},"source":["### Continuação: Pytorch e o caso de estudo LeNet-5\n","\n","Vamos agora definir explicitamente o *padding* e *stride* em cada camada convolucional.\n","Como, por padrão, o *padding* e *stride* dessa camada é 0 e 1, o nosso resultado ainda não vai mudar. Entretanto, agora podemos calcular a saída exata de cada camada.\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://miro.medium.com/max/2625/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n","</p>\n","\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://engmrk.com/wp-content/uploads/2018/09/LeNEt_Summary_Table.jpg\">\n","</p>\n"]},{"cell_type":"code","metadata":{"id":"L7v3tbtLzsgM","colab_type":"code","outputId":"c2a7164c-e87c-4885-fdf3-2c50af38579e","executionInfo":{"status":"ok","timestamp":1570924285533,"user_tz":180,"elapsed":439112,"user":{"displayName":"Felipe Marcelino","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3yhjltVDWqNXwhUpquSUaO9nIa2J0oIFCyJy6VQ=s64","userId":"10589390457938412332"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n","# tamanho do batch, e lambda do weight decay\n","num_epochs, lr, batch_size, wd_lambda = 10, 0.1, 128, 0.000001\n","\n","# rede baseada na LeNet-5 \n","net = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=6, kernel_size=5, stride=1, padding=0),     # entrada: (b, 1, 32, 32) e saida: (b, 6, 28, 28)\n","        nn.Tanh(),\n","        nn.Conv2d(in_channels=6,out_channels=16, kernel_size=5, stride=1, padding=0),    # entrada: (b, 6, 28, 28) e saida: (b, 16, 24, 24)\n","        nn.Tanh(),\n","        nn.Conv2d(in_channels=16,out_channels=120, kernel_size=5, stride=1, padding=0),   # entrada: (b, 16, 24, 24) e saida: (b, 120, 20, 20)\n","        nn.Tanh(),\n","        nn.Flatten(),  # lineariza formando um vetor                              # entrada: (b, 120, 20, 20) e saida: (b, 120*20*20) = (b, 48000)\n","        nn.Linear(48000, 84),                                          # entrada: (b, 48000) e saida: (b, 84)\n","        nn.Tanh(),\n","        nn.Linear(84, 10))                                                             # entrada: (b, 84) e saida: (b, 10)\n","\n","# Sending model to device\n","net.to(device)\n","\n","# função de custo (ou loss)\n","loss = nn.CrossEntropyLoss()\n","\n","# carregamento do dado: mnist\n","train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=32)\n","\n","# trainer do gluon\n","trainer = optim.SGD(net.parameters(), lr=lr, weight_decay=wd_lambda)\n","\n","# treinamento e validação via Pytorch\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","                num_epochs)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["training on cuda\n","epoch 1, train loss 0.5789, train acc 0.788, test loss 0.4645, test acc 0.828, time 14.1 sec\n","epoch 2, train loss 0.4043, train acc 0.855, test loss 0.3929, test acc 0.857, time 14.1 sec\n","epoch 3, train loss 0.3563, train acc 0.872, test loss 0.3747, test acc 0.864, time 14.1 sec\n","epoch 4, train loss 0.3224, train acc 0.883, test loss 0.3545, test acc 0.870, time 14.0 sec\n","epoch 5, train loss 0.2977, train acc 0.891, test loss 0.3609, test acc 0.867, time 14.1 sec\n","epoch 6, train loss 0.2787, train acc 0.899, test loss 0.3268, test acc 0.879, time 14.1 sec\n","epoch 7, train loss 0.2633, train acc 0.903, test loss 0.3374, test acc 0.875, time 14.1 sec\n","epoch 8, train loss 0.2460, train acc 0.911, test loss 0.3295, test acc 0.878, time 13.8 sec\n","epoch 9, train loss 0.2320, train acc 0.915, test loss 0.3201, test acc 0.883, time 14.1 sec\n","epoch 10, train loss 0.2170, train acc 0.921, test loss 0.3207, test acc 0.881, time 14.1 sec\n"],"name":"stdout"}]}]}